{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a4307e",
   "metadata": {},
   "source": [
    "# Comprehensive Workflow for ATS Input Generation for Coweeta: Part 2 - Surface/Subsurface Properties and Mesh Extrusion\n",
    " \n",
    "This workflow illustrates the process of developing a simulation campaign for integrated hydrology using ATS.\n",
    " \n",
    "### Overview\n",
    " \n",
    "In Part 2 of this workflow, we focus on mapping datasets onto a mesh to define surface and subsurface properties, documented as labeled sets. The final step involves extruding the mesh into a 3D `m3` mesh object, which is then exported as an Exodus file.\n",
    "\n",
    "### Datasets Used\n",
    " \n",
    "- `NLCD`: land cover/transpiration/rooting depths\n",
    "- `MODIS`: LAI\n",
    "- `GLYHMPS`: geology data for structural formations\n",
    "- `Pelletier`: depth to bedrock and soil texture information\n",
    "- `SSURGO`: soil data, where available, in the top 2m.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc55133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these can be turned on for development work\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3040cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIX ME -- why is this broken without importing netcdf first?\n",
    "import netCDF4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up logging first or else it gets preempted by another package\n",
    "import watershed_workflow.ui\n",
    "watershed_workflow.ui.setup_logging(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed16a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import logging\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import cftime, datetime\n",
    "\n",
    "\n",
    "import watershed_workflow.io\n",
    "import watershed_workflow.land_cover_properties\n",
    "import watershed_workflow.regions\n",
    "# import watershed_workflow.sources.standard_names as names\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# set the default figure size for notebooks\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a04710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force Watershed Workflow to pull data from this directory rather than a shared data directory.\n",
    "# This picks up the Coweeta-specific datasets set up here to avoid large file downloads for \n",
    "# demonstration purposes.\n",
    "#\n",
    "def splitPathFull(path):\n",
    "    \"\"\"\n",
    "    Splits an absolute path into a list of components such that\n",
    "    os.path.join(*splitPathFull(path)) == path\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    while True:\n",
    "        head, tail = os.path.split(path)\n",
    "        if head == path:  # root on Unix or drive letter with backslash on Windows (e.g., C:\\)\n",
    "            parts.insert(0, head)\n",
    "            break\n",
    "        elif tail == path:  # just a single file or directory\n",
    "            parts.insert(0, tail)\n",
    "            break\n",
    "        else:\n",
    "            parts.insert(0, tail)\n",
    "            path = head\n",
    "    return parts\n",
    "\n",
    "cwd = splitPathFull(os.getcwd())\n",
    "\n",
    "# # REMOVE THIS PORTION OF THE CELL for general use outside of Coweeta -- this is just locating \n",
    "# # the working directory within the WW directory structure\n",
    "# if cwd[-1] == 'Coweeta':\n",
    "#     pass\n",
    "# elif cwd[-1] == 'examples':\n",
    "#     cwd.append('Coweeta')\n",
    "# else:\n",
    "#     cwd.extend(['examples','Coweeta'])\n",
    "# # END REMOVE THIS PORTION\n",
    "\n",
    "# Note, this directory is where downloaded data will be put as well\n",
    "data_dir = os.path.join(*(cwd + ['input_data',]))\n",
    "def toInput(filename):\n",
    "    return os.path.join(data_dir, filename)\n",
    "\n",
    "output_dir = os.path.join(*(cwd + ['output_data',]))\n",
    "def toOutput(filename):\n",
    "    return os.path.join(output_dir, filename)\n",
    "\n",
    "work_dir = os.path.join(*cwd)\n",
    "def toWorkingDir(filename):\n",
    "    return os.path.join(work_dir, filename)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory to the local space to get the locally downloaded files\n",
    "# REMOVE THIS CELL for general use outside fo Coweeta\n",
    "watershed_workflow.config.setDataDirectory(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c5834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Coweeta'\n",
    "\n",
    "# Simulation control\n",
    "# - note that we use the NoLeap calendar, same as DayMet.  Simulations are typically run over the \"water year\"\n",
    "#   which starts August 1.\n",
    "start = cftime.DatetimeNoLeap(2010,8,1)\n",
    "end = cftime.DatetimeNoLeap(2011,8,1)\n",
    "\n",
    "nyears_cyclic_steadystate = 4   # how many years to run spinup\n",
    "\n",
    "# Global Soil Properties\n",
    "min_porosity = 0.05 # minimum porosity considered \"too small\"\n",
    "max_permeability = 1.e-10 # max value considered \"too permeable\"\n",
    "max_vg_alpha = 1.e-3 # max value of van Genuchten's alpha -- our correlation is not valid for some soils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67d7707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a dictionary of source objects\n",
    "#\n",
    "# Data sources, also called managers, deal with downloading and parsing data files from a variety of online APIs.\n",
    "sources = watershed_workflow.sources.getDefaultSources()\n",
    "sources['hydrography'] = watershed_workflow.sources.hydrography_sources['NHDPlus HR']\n",
    "\n",
    "#\n",
    "# This demo uses a few datasets that have been clipped out of larger, national\n",
    "# datasets and are distributed with the code.  This is simply to save download\n",
    "# time for this simple problem and to lower the barrier for trying out\n",
    "# Watershed Workflow.  A more typical workflow would delete these lines (as \n",
    "# these files would not exist for other watersheds).\n",
    "#\n",
    "# The default versions of these download large raster and shapefile files that\n",
    "# are defined over a very large region (globally or the entire US).\n",
    "#\n",
    "# DELETE THIS SECTION for non-Coweeta runs\n",
    "dtb_file = os.path.join(data_dir, 'DTB', 'DTB.tif')\n",
    "geo_file = os.path.join(data_dir, 'GLHYMPS', 'GLHYMPS.shp')\n",
    "\n",
    "# GLHYMPs is a several-GB download, so we have sliced it and included the slice here\n",
    "sources['geologic structure'] = watershed_workflow.sources.ManagerGLHYMPS(geo_file)\n",
    "\n",
    "# The Pelletier DTB map is not particularly accurate at Coweeta -- the SoilGrids map seems to be better.\n",
    "# Here we will use a clipped version of that map.\n",
    "sources['depth to bedrock'] = watershed_workflow.sources.ManagerRaster(dtb_file)\n",
    "\n",
    "# END DELETE THIS SECTION\n",
    "\n",
    "# log the sources that will be used here\n",
    "watershed_workflow.sources.logSources(sources)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary of output_filenames -- will include all filenames generated\n",
    "output_filenames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae8d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that, by default, we tend to work in the DayMet CRS because this allows us to avoid\n",
    "# reprojecting meteorological forcing datasets.\n",
    "crs = watershed_workflow.crs.daymet_crs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ab257c",
   "metadata": {},
   "source": [
    "### Load Pickle Files from Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49db680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_dir = './intermediate_files/'\n",
    "\n",
    "# Load the parquet file\n",
    "rivers = pd.read_parquet(f'{intermediate_dir}rivers.parquet')\n",
    "\n",
    "# Load the pickle files\n",
    "with open(f'{intermediate_dir}m2.pkl', 'rb') as file:\n",
    "    m2 = pickle.load(file)\n",
    "\n",
    "with open(f'{intermediate_dir}watershed.pkl', 'rb') as file:\n",
    "    watershed = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3fd099",
   "metadata": {},
   "source": [
    "# Surface properties\n",
    "\n",
    "Meshes interact with data to provide forcing, parameters, and more in the actual simulation.  Specifically, we need vegetation type on the surface to provide information about transpiration and subsurface structure to provide information about water retention curves, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc844a5e",
   "metadata": {},
   "source": [
    "## NLCD for LULC\n",
    "\n",
    "We'll start by downloading and collecting land cover from the NLCD dataset, and generate sets for each land cover type that cover the surface.  Likely these will be some combination of grass, deciduous forest, coniferous forest, and mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c406e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the NLCD raster\n",
    "nlcd = sources['land cover'].getDataset(watershed.exterior.buffer(100), watershed.crs)['cover']\n",
    "\n",
    "# what land cover types did we get?\n",
    "logging.info('Found land cover dtypes: {}'.format(nlcd.dtype))\n",
    "logging.info('Found land cover types: {}'.format(set(list(nlcd.values.ravel()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75361648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a colormap for the data\n",
    "nlcd_indices, nlcd_cmap, nlcd_norm, nlcd_ticks, nlcd_labels = \\\n",
    "      watershed_workflow.colors.createNLCDColormap(np.unique(nlcd))\n",
    "nlcd_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(4, 4))\n",
    "nlcd.plot.imshow(ax=ax, cmap=nlcd_cmap, norm=nlcd_norm, add_colorbar=False)\n",
    "watershed_workflow.colors.createIndexedColorbar(ncolors=len(nlcd_indices), \n",
    "                               cmap=nlcd_cmap, labels=nlcd_labels, ax=ax) \n",
    "ax.set_title('Land Cover')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7bc0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map nlcd onto the mesh\n",
    "m2_nlcd = watershed_workflow.getDatasetOnMesh(m2, nlcd, method='nearest')\n",
    "m2.cell_data = pd.DataFrame({'land_cover': m2_nlcd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double-check that nan not in the values\n",
    "assert 127 not in m2_nlcd\n",
    "\n",
    "# create a new set of labels and indices with only those that actually appear on the mesh\n",
    "nlcd_indices, nlcd_cmap, nlcd_norm, nlcd_ticks, nlcd_labels = \\\n",
    "      watershed_workflow.colors.createNLCDColormap(np.unique(m2_nlcd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04326dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "mp = m2.plot(facecolors=m2_nlcd, cmap=nlcd_cmap, norm=nlcd_norm, edgecolors=None, colorbar=False, ax=ax)\n",
    "watershed_workflow.colors.createIndexedColorbar(ncolors=len(nlcd_indices), \n",
    "                               cmap=nlcd_cmap, labels=nlcd_labels, ax=ax) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb56a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labeled sets to the mesh for NLCD\n",
    "nlcd_labels_dict = dict(zip(nlcd_indices, nlcd_labels))\n",
    "watershed_workflow.regions.addSurfaceRegions(m2, names=nlcd_labels_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlcd_labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62bb80d-d96f-4468-9195-96f51bcbd6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filenames['nlcd_indices'] = [int(i) for i in nlcd_indices]\n",
    "output_filenames['nlcd_labels'] = nlcd_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f37fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ls in m2.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc606f",
   "metadata": {},
   "source": [
    "## MODIS LAI\n",
    "\n",
    "Leaf area index is needed on each land cover type -- this is used in the Evapotranspiration calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a1f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download LAI and corresponding LULC datasets -- these are actually already downloaded, \n",
    "# as the MODIS AppEEARS API is quite slow\n",
    "#\n",
    "# Note that MODIS does NOT work with the noleap calendar, so we have to convert to actual dates first\n",
    "start_leap = cftime.DatetimeGregorian(start.year, start.month, start.day)\n",
    "end_leap = cftime.DatetimeGregorian(end.year, end.month, end.day)\n",
    "modis_data = sources['LAI'].getDataset(watershed.exterior, crs, start_leap, end_leap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4a0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIS data comes with time-dependent LAI AND time-dependent LULC -- just take the mode to find the most common LULC\n",
    "modis_data['LULC'] = watershed_workflow.data.computeMode(modis_data['LULC'], 'time_LULC')\n",
    "\n",
    "# now it is safe to have only one time\n",
    "modis_data = modis_data.rename({'time_LAI':'time'})\n",
    "\n",
    "# remove leap day (366th day of any leap year) to match our Noleap Calendar\n",
    "modis_data = watershed_workflow.data.filterLeapDay(modis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d001cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the MODIS data -- note the entire domain is covered with one type for Coweeta (it is small!)\n",
    "modis_data['LULC'].plot.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the transient time series\n",
    "modis_lai = watershed_workflow.land_cover_properties.computeTimeSeries(modis_data['LAI'], modis_data['LULC'], \n",
    "                                                                      polygon=watershed.exterior, polygon_crs=watershed.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae92b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_lai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b525c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smooth the data in time\n",
    "modis_lai_smoothed = watershed_workflow.data.smoothTimeSeries(modis_lai, 'time')\n",
    "\n",
    "# save the MODIS time series to disk\n",
    "output_filenames['modis_lai_transient'] = toOutput(f'{name}_LAI_MODIS_transient.h5')\n",
    "watershed_workflow.io.writeTimeseriesToHDF5(output_filenames['modis_lai_transient'], modis_lai_smoothed)\n",
    "watershed_workflow.land_cover_properties.plotLAI(modis_lai_smoothed, indices='MODIS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e44a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute a typical year\n",
    "modis_lai_typical = watershed_workflow.data.computeAverageYear(modis_lai_smoothed, 'time', output_nyears=10, \n",
    "                                                                  start_year=2000)\n",
    "\n",
    "output_filenames['modis_lai_cyclic_steadystate'] = toOutput(f'{name}_LAI_MODIS_CyclicSteadystate.h5')\n",
    "watershed_workflow.io.writeTimeseriesToHDF5(output_filenames['modis_lai_cyclic_steadystate'], modis_lai_typical)\n",
    "watershed_workflow.land_cover_properties.plotLAI(modis_lai_typical, indices='MODIS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d0adba",
   "metadata": {},
   "source": [
    "## Crosswalk of LAI to NLCD LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e14d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosswalk = watershed_workflow.land_cover_properties.computeCrosswalk(modis_data['LULC'], nlcd, method='fractional area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5822464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the NLCD-based time series\n",
    "nlcd_lai_cyclic_steadystate = watershed_workflow.land_cover_properties.applyCrosswalk(crosswalk, modis_lai_typical)\n",
    "nlcd_lai_transient = watershed_workflow.land_cover_properties.applyCrosswalk(crosswalk, modis_lai_smoothed)\n",
    "\n",
    "watershed_workflow.land_cover_properties.removeNullLAI(nlcd_lai_cyclic_steadystate)\n",
    "watershed_workflow.land_cover_properties.removeNullLAI(nlcd_lai_transient)\n",
    "nlcd_lai_transient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the NLCD-based time series to disk\n",
    "output_filenames['nlcd_lai_cyclic_steadystate'] = toOutput(f'{name}_LAI_NLCD_CyclicSteadystate.h5')\n",
    "watershed_workflow.io.writeTimeseriesToHDF5(output_filenames['nlcd_lai_cyclic_steadystate'], nlcd_lai_cyclic_steadystate)\n",
    "\n",
    "output_filenames['nlcd_lai_transient'] = toOutput(f'{name}_LAI_NLCD_{start.year}_{end.year}.h5')\n",
    "watershed_workflow.io.writeTimeseriesToHDF5(output_filenames['nlcd_lai_transient'], nlcd_lai_transient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414baf6c",
   "metadata": {},
   "source": [
    "# Subsurface Soil, Geologic Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4beee",
   "metadata": {},
   "source": [
    "## NRCS Soils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89adf978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get NRCS shapes, on a reasonable crs\n",
    "nrcs = sources['soil structure'].getShapesByGeometry(watershed.exterior, watershed.crs).to_crs(crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53941ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a clean dataframe with just the data we will need for ATS\n",
    "def replace_column_nans(df, col_nan, col_replacement):\n",
    "    \"\"\"In a df, replace col_nan entries by col_replacement if is nan.  In Place!\"\"\"\n",
    "    row_indexer = df[col_nan].isna()\n",
    "    df.loc[row_indexer, col_nan] = df.loc[row_indexer, col_replacement]\n",
    "    return\n",
    "\n",
    "# where poro or perm is nan, put Rosetta poro\n",
    "replace_column_nans(nrcs, 'porosity [-]', 'Rosetta porosity [-]')\n",
    "replace_column_nans(nrcs, 'permeability [m^2]', 'Rosetta permeability [m^2]')\n",
    "\n",
    "# drop unnecessary columns\n",
    "for col in ['Rosetta porosity [-]', 'Rosetta permeability [m^2]', 'bulk density [g/cm^3]', 'total sand pct [%]',\n",
    "            'total silt pct [%]', 'total clay pct [%]']:\n",
    "    nrcs.pop(col)\n",
    "    \n",
    "# drop nans\n",
    "nan_mask = nrcs.isna().any(axis=1)\n",
    "dropped_mukeys = nrcs.index[nan_mask]\n",
    "\n",
    "# Drop those rows\n",
    "nrcs = nrcs[~nan_mask]\n",
    "\n",
    "assert nrcs['porosity [-]'][:].min() >= min_porosity\n",
    "assert nrcs['permeability [m^2]'][:].max() <= max_permeability\n",
    "nrcs\n",
    "\n",
    "# check for nans\n",
    "nrcs.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd38beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the soil color of each cell of the mesh\n",
    "# Note, we use mukey here because it is an int, while ID is a string\n",
    "soil_color_mukey = watershed_workflow.getShapePropertiesOnMesh(m2, nrcs, 'mukey', \n",
    "                                                         resolution=50, nodata=-999)\n",
    "\n",
    "nrcs.set_index('mukey', drop=False, inplace=True)\n",
    "\n",
    "unique_soil_colors = list(np.unique(soil_color_mukey))\n",
    "if -999 in unique_soil_colors:\n",
    "    unique_soil_colors.remove(-999)\n",
    "\n",
    "# retain only the unique values of soil_color\n",
    "nrcs = nrcs.loc[unique_soil_colors]\n",
    "\n",
    "# renumber the ones we know will appear with an ATS ID using ATS conventions\n",
    "nrcs['ATS ID'] = range(1000, 1000+len(unique_soil_colors))\n",
    "nrcs.set_index('ATS ID', drop=True, inplace=True)\n",
    "\n",
    "# create a new soil color and a soil thickness map using the ATS IDs\n",
    "soil_color = -np.ones_like(soil_color_mukey)\n",
    "soil_thickness = np.nan * np.ones(soil_color.shape, 'd')\n",
    "\n",
    "for ats_ID, ID, thickness in zip(nrcs.index, nrcs.mukey, nrcs['thickness [m]']):\n",
    "    mask = np.where(soil_color_mukey == ID)\n",
    "    soil_thickness[mask] = thickness\n",
    "    soil_color[mask] = ats_ID\n",
    "\n",
    "m2.cell_data['soil_color'] = soil_color\n",
    "m2.cell_data['soil thickness'] = soil_thickness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba4ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the soil color\n",
    "# -- get a cmap for soil color\n",
    "sc_indices, sc_cmap, sc_norm, sc_ticks, sc_labels = \\\n",
    "      watershed_workflow.colors.createIndexedColormap(nrcs.index)\n",
    "\n",
    "mp = m2.plot(facecolors=m2.cell_data['soil_color'], cmap=sc_cmap, norm=sc_norm, edgecolors=None, colorbar=False)\n",
    "watershed_workflow.colors.createIndexedColorbar(ncolors=len(nrcs), \n",
    "                               cmap=sc_cmap, labels=sc_labels, ax=plt.gca()) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f46a38a",
   "metadata": {},
   "source": [
    "## Depth to Bedrock from SoilGrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e00ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtb = sources['depth to bedrock'].getDataset(watershed.exterior, watershed.crs)['band_1']\n",
    "\n",
    "# the SoilGrids dataset is in cm --> convert to meters\n",
    "dtb.values = dtb.values/100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6d1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map to the mesh\n",
    "m2.cell_data['dtb'] = watershed_workflow.getDatasetOnMesh(m2, dtb, method='linear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc3b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gons = m2.plot(facecolors=m2.cell_data['dtb'], cmap='RdBu', edgecolors=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cbb4c",
   "metadata": {},
   "source": [
    "## GLHYMPs Geology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "glhymps = sources['geologic structure'].getShapesByGeometry(watershed.exterior.buffer(1000), watershed.crs, out_crs=crs)\n",
    "glhymps = watershed_workflow.soil_properties.mangleGLHYMPSProperties(glhymps,\n",
    "                                              min_porosity=min_porosity, \n",
    "                                              max_permeability=max_permeability, \n",
    "                                              max_vg_alpha=max_vg_alpha)\n",
    "\n",
    "# intersect with the buffered geometry -- don't keep extras\n",
    "glhymps = glhymps[glhymps.intersects(watershed.exterior.buffer(10))]\n",
    "glhymps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a25e138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quality check -- make sure glymps shapes cover the watershed\n",
    "print(glhymps.union_all().contains(watershed.exterior))\n",
    "glhymps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8571ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "glhymps.pop('logk_stdev [-]')\n",
    "\n",
    "assert glhymps['porosity [-]'][:].min() >= min_porosity\n",
    "assert glhymps['permeability [m^2]'][:].max() <= max_permeability\n",
    "assert glhymps['van Genuchten alpha [Pa^-1]'][:].max() <= max_vg_alpha\n",
    "\n",
    "glhymps.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa6be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that for larger areas there are often common regions -- two labels with the same properties -- no need to duplicate those with identical values.\n",
    "def reindex_remove_duplicates(df, index):\n",
    "    \"\"\"Removes duplicates, creating a new index and saving the old index as tuples of duplicate values. In place!\"\"\"\n",
    "    if index is not None:\n",
    "        if index in df:\n",
    "            df.set_index(index, drop=True, inplace=True)\n",
    "    \n",
    "    index_name = df.index.name\n",
    "\n",
    "    # identify duplicate rows\n",
    "    duplicates = list(df.groupby(list(df)).apply(lambda x: tuple(x.index)))\n",
    "\n",
    "    # order is preserved\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df[index_name] = duplicates\n",
    "    return\n",
    "\n",
    "reindex_remove_duplicates(glhymps, 'ID')\n",
    "glhymps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3bc580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the geo color of each cell of the mesh\n",
    "geology_color_glhymps = watershed_workflow.getShapePropertiesOnMesh(m2, glhymps, 'index', \n",
    "                                                         resolution=50, nodata=-999)\n",
    "\n",
    "# retain only the unique values of geology that actually appear in our cell mesh\n",
    "unique_geology_colors = list(np.unique(geology_color_glhymps))\n",
    "if -999 in unique_geology_colors:\n",
    "    unique_geology_colors.remove(-999)\n",
    "\n",
    "# retain only the unique values of geology_color\n",
    "glhymps = glhymps.loc[unique_geology_colors]\n",
    "\n",
    "# renumber the ones we know will appear with an ATS ID using ATS conventions\n",
    "glhymps['ATS ID'] = range(100, 100+len(unique_geology_colors))\n",
    "glhymps['TMP_ID'] = glhymps.index\n",
    "glhymps.reset_index(drop=True, inplace=True)\n",
    "glhymps.set_index('ATS ID', drop=True, inplace=True)\n",
    "\n",
    "# create a new geology color using the ATS IDs\n",
    "geology_color = -np.ones_like(geology_color_glhymps)\n",
    "for ats_ID, tmp_ID in zip(glhymps.index, glhymps.TMP_ID):\n",
    "    geology_color[np.where(geology_color_glhymps == tmp_ID)] = ats_ID\n",
    "\n",
    "glhymps.pop('TMP_ID')\n",
    "\n",
    "m2.cell_data['geology_color'] = geology_color\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a492a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "geology_color_glhymps.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14792bea",
   "metadata": {},
   "source": [
    "## Combine to form a complete subsurface dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f013f7c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bedrock = watershed_workflow.soil_properties.getDefaultBedrockProperties()\n",
    "\n",
    "# merge the properties databases\n",
    "subsurface_props = pd.concat([glhymps, nrcs, bedrock])\n",
    "\n",
    "# save the properties to disk for use in generating input file\n",
    "output_filenames['subsurface_properties'] = toOutput(f'{name}_subsurface_properties.csv')\n",
    "subsurface_props.to_csv(output_filenames['subsurface_properties'])\n",
    "subsurface_props\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a191f2",
   "metadata": {},
   "source": [
    "# Extrude the 2D Mesh to make a 3D mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d498ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the floor of the domain as max DTB\n",
    "dtb_max = np.nanmax(m2.cell_data['dtb'].values)\n",
    "m2.cell_data['dtb'] = m2.cell_data['dtb'].fillna(dtb_max)\n",
    "\n",
    "print(f'total thickness: {dtb_max} m')\n",
    "total_thickness = 50.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dz structure for the top 2m of soil\n",
    "#\n",
    "# here we try for 10 cells, starting at 5cm at the top and going to 50cm at the bottom of the 2m thick soil\n",
    "dzs, res = watershed_workflow.mesh.optimizeDzs(0.05, 0.5, 2, 10)\n",
    "print(dzs)\n",
    "print(sum(dzs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c18ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks like it would work out, with rounder numbers:\n",
    "dzs_soil = [0.05, 0.05, 0.05, 0.12, 0.23, 0.5, 0.5, 0.5]\n",
    "print(sum(dzs_soil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4552c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50m total thickness, minus 2m soil thickness, leaves us with 48 meters to make up.\n",
    "# optimize again...\n",
    "dzs2, res2 = watershed_workflow.mesh.optimizeDzs(1, 10, 48, 8)\n",
    "print(dzs2)\n",
    "print(sum(dzs2))\n",
    "\n",
    "# how about...\n",
    "dzs_geo = [1.0, 2.0, 4.0, 8.0, 11, 11, 11]\n",
    "print(dzs_geo)\n",
    "print(sum(dzs_geo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer extrusion\n",
    "DTB = m2.cell_data['dtb'].values\n",
    "soil_color = m2.cell_data['soil_color'].values\n",
    "geo_color = m2.cell_data['geology_color'].values\n",
    "soil_thickness = m2.cell_data['soil thickness'].values\n",
    "\n",
    "\n",
    "# -- data structures needed for extrusion\n",
    "layer_types = []\n",
    "layer_data = []\n",
    "layer_ncells = []\n",
    "layer_mat_ids = []\n",
    "\n",
    "# -- soil layer --\n",
    "depth = 0\n",
    "for dz in dzs_soil:\n",
    "    depth += 0.5 * dz\n",
    "    layer_types.append('constant')\n",
    "    layer_data.append(dz)\n",
    "    layer_ncells.append(1)\n",
    "    \n",
    "    # use glhymps params\n",
    "    br_or_geo = np.where(depth < DTB, geo_color, 999)\n",
    "    soil_or_br_or_geo = np.where(np.bitwise_and(soil_color > 0, depth < soil_thickness),\n",
    "                                 soil_color,\n",
    "                                 br_or_geo)\n",
    "\n",
    "    layer_mat_ids.append(soil_or_br_or_geo)\n",
    "    depth += 0.5 * dz\n",
    "    \n",
    "# -- geologic layer --\n",
    "for dz in dzs_geo:\n",
    "    depth += 0.5 * dz\n",
    "    layer_types.append('constant')\n",
    "    layer_data.append(dz)\n",
    "    layer_ncells.append(1)\n",
    "    \n",
    "    geo_or_br = np.where(depth < DTB, geo_color, 999)\n",
    "\n",
    "    layer_mat_ids.append(geo_or_br)\n",
    "    depth += 0.5 * dz\n",
    "\n",
    "# print the summary\n",
    "watershed_workflow.mesh.Mesh3D.summarizeExtrusion(layer_types, layer_data, \n",
    "                                            layer_ncells, layer_mat_ids)\n",
    "\n",
    "# downselect subsurface properties to only those that are used\n",
    "layer_mat_id_used = list(np.unique(np.array(layer_mat_ids)))\n",
    "subsurface_props_used = subsurface_props.loc[layer_mat_id_used]\n",
    "subsurface_props_used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11789aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrude\n",
    "m3 = watershed_workflow.mesh.Mesh3D.extruded_Mesh2D(m2, layer_types, layer_data, \n",
    "                                             layer_ncells, layer_mat_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd726709",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('2D labeled sets')\n",
    "print('---------------')\n",
    "for ls in m2.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')\n",
    "\n",
    "print('')\n",
    "print('Extruded 3D labeled sets')\n",
    "print('------------------------')\n",
    "for ls in m3.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')\n",
    "\n",
    "print('')\n",
    "print('Extruded 3D side sets')\n",
    "print('---------------------')\n",
    "for ls in m3.side_sets:\n",
    "    print(f'{ls.setid} : FACE : {len(ls.cell_list)} : \"{ls.name}\"')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e74660d-6782-48ed-af2e-b3f8e7a099aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(intermediate_dir, 'm3.pkl'), 'wb') as f:\n",
    "    pickle.dump(m3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the mesh to disk\n",
    "output_filenames['mesh'] = toOutput(f'{name}.exo')\n",
    "try:\n",
    "    os.remove(output_filenames['mesh'])\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "m3.writeExodus(output_filenames['mesh'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38740dcd",
   "metadata": {},
   "source": [
    "<b> This concludes part 2 of the workflow, where surface and subsurface properties are defined, and the 3D mesh is extruded and labeled. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(output_filenames)\n",
    "\n",
    "out_files_path = os.path.join(intermediate_dir, 'output_files.pkl')\n",
    "with open(out_files_path, 'wb') as f:\n",
    "    pickle.dump(output_filenames, f)\n",
    "    \n",
    "    \n",
    "print(\"Intermediate files saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50e9c9-9e0c-46ba-a40e-0c6a53903ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python3 (watershed_workflow)",
   "language": "python",
   "name": "watershed_workflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 685.28134,
   "end_time": "2022-03-07T16:00:23.770205",
   "environment_variables": {},
   "exception": true,
   "input_path": "full_workflow_master.ipynb",
   "output_path": "full_workflow_EastTaylor.ipynb",
   "parameters": {
    "hucs": "[14020001,]",
    "name": "EastTaylor",
    "prune_by_area_fraction": 0.005
   },
   "start_time": "2022-03-07T15:48:58.488865",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
