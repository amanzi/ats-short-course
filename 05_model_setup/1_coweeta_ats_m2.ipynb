{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c4f8ea",
   "metadata": {
    "papermill": {
     "duration": 0.155249,
     "end_time": "2022-03-07T15:48:59.904477",
     "exception": false,
     "start_time": "2022-03-07T15:48:59.749228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Complete Workflow for generating ATS input for Coweeta: Part 1 - Surface Meshing\n",
    " \n",
    "This workflow demonstrates how to develop a simulation campaign for integrated hydrology using ATS. \n",
    " \n",
    "### Overview\n",
    " \n",
    "Part 1 of this workflow focuses on creating a terrain-following surface mesh. It uses arbitrary polygonal shapes to generate computational elements that align with key watershed features. For instance, elongated quadrilaterals and pentagons at junctions are aligned with stream centerlines to efficiently resolve stream networks.\n",
    " \n",
    "The notebook processes hydrography data to meet tessellation requirements and performs tessellation under specific constraints. It also incorporates hydrologic conditioning of the surface topography within the mesh. The resulting surface mesh is stored as an `m2` class object and saved as a pickle file for use in Part 2.\n",
    "\n",
    "### Datasets Used\n",
    " \n",
    "- `NHD Plus`: Hydrography\n",
    "- `3DEP`: Elevation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc581b68-0592-42aa-8bee-71a913ca7fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up logging first or else it gets preempted by another package\n",
    "import watershed_workflow.ui\n",
    "watershed_workflow.ui.setup_logging(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1705a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import shapely\n",
    "import pandas as pd\n",
    "\n",
    "import watershed_workflow \n",
    "import watershed_workflow.config\n",
    "import watershed_workflow.sources\n",
    "import watershed_workflow.utils\n",
    "import watershed_workflow.plot\n",
    "import watershed_workflow.mesh\n",
    "import watershed_workflow.regions\n",
    "import watershed_workflow.sources.standard_names as names\n",
    "\n",
    "\n",
    "# set the default figure size for notebooks\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f9c839-7706-4428-8976-d90c9064afd0",
   "metadata": {},
   "source": [
    "# Input: Parameters and other source data\n",
    "\n",
    "Note, this section will need to be modified for other runs of this workflow in other regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742c884a-be3a-4f34-bd1c-ec5c605bcd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force Watershed Workflow to pull data from this directory rather than a shared data directory.\n",
    "# This picks up the Coweeta-specific datasets set up here to avoid large file downloads for \n",
    "# demonstration purposes.\n",
    "#\n",
    "def splitPathFull(path):\n",
    "    \"\"\"\n",
    "    Splits an absolute path into a list of components such that\n",
    "    os.path.join(*splitPathFull(path)) == path\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    while True:\n",
    "        head, tail = os.path.split(path)\n",
    "        if head == path:  # root on Unix or drive letter with backslash on Windows (e.g., C:\\)\n",
    "            parts.insert(0, head)\n",
    "            break\n",
    "        elif tail == path:  # just a single file or directory\n",
    "            parts.insert(0, tail)\n",
    "            break\n",
    "        else:\n",
    "            parts.insert(0, tail)\n",
    "            path = head\n",
    "    return parts\n",
    "\n",
    "cwd = splitPathFull(os.getcwd())\n",
    "\n",
    "# Note, this directory is where downloaded data will be put as well\n",
    "data_dir = os.path.join(*(cwd + ['input_data',]))\n",
    "def toInput(filename):\n",
    "    return os.path.join(data_dir, filename)\n",
    "\n",
    "output_dir = os.path.join(*(cwd + ['output_data',]))\n",
    "def toOutput(filename):\n",
    "    return os.path.join(output_dir, filename)\n",
    "\n",
    "work_dir = os.path.join(*cwd)\n",
    "def toWorkingDir(filename):\n",
    "    return os.path.join(work_dir, filename)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87aad6a-b0cb-4a23-8f3c-625191b2a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory to the local space to get the locally downloaded files\n",
    "# REMOVE THIS CELL for general use outside fo Coweeta\n",
    "watershed_workflow.config.setDataDirectory(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403772e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters cell -- this provides all parameters that can be changed via pipelining to generate a new watershed. \n",
    "name = 'Coweeta'\n",
    "coweeta_shapefile = os.path.join('input_data', 'coweeta_basin.shp')\n",
    "\n",
    "# Geometric parameters\n",
    "# -- parameters to clean and reduce the river network prior to meshing\n",
    "simplify = 60                   # length scale to target average edge \n",
    "ignore_small_rivers = 2         # remove rivers with fewer than this number of reaches -- important for NHDPlus HR \n",
    "prune_by_area_fraction = 0.01   # prune any reaches whose contributing area is less than this fraction of the domain\n",
    "\n",
    "# -- mesh triangle refinement control\n",
    "refine_d0 = 200\n",
    "refine_d1 = 600\n",
    "\n",
    "refine_L0 = 125\n",
    "refine_L1 = 300\n",
    "\n",
    "refine_A0 = refine_L0**2 / 2\n",
    "refine_A1 = refine_L1**2 / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9695d127",
   "metadata": {
    "papermill": {
     "duration": 0.152343,
     "end_time": "2022-03-07T15:49:01.391199",
     "exception": false,
     "start_time": "2022-03-07T15:49:01.238856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a dictionary of pickle filenames -- will include all pickle files generated\n",
    "pickle_filenames = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93821426",
   "metadata": {
    "papermill": {
     "duration": 0.169238,
     "end_time": "2022-03-07T15:49:05.337830",
     "exception": false,
     "start_time": "2022-03-07T15:49:05.168592",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note that, by default, we tend to work in the DayMet CRS because this allows us to avoid\n",
    "# reprojecting meteorological forcing datasets.\n",
    "crs = watershed_workflow.crs.daymet_crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05763672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape and crs of the shape\n",
    "coweeta_mgr = watershed_workflow.sources.ManagerShapefile(coweeta_shapefile)\n",
    "coweeta = coweeta_mgr.getShapes(out_crs=crs)\n",
    "coweeta.rename(columns={'AREA' : names.AREA, 'LABEL' : names.NAME}, inplace=True)\n",
    "coweeta[names.ID] = coweeta.index.values\n",
    "coweeta.set_index(names.ID, inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8204e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(coweeta)) \n",
    "# display(coweeta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17feb6e0",
   "metadata": {
    "papermill": {
     "duration": 0.181137,
     "end_time": "2022-03-07T15:49:06.850648",
     "exception": false,
     "start_time": "2022-03-07T15:49:06.669511",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up a dictionary of source objects\n",
    "#\n",
    "# Data sources, also called managers, deal with downloading and parsing data files from a variety of online APIs.\n",
    "sources = watershed_workflow.sources.getDefaultSources()\n",
    "sources['hydrography'] = watershed_workflow.sources.hydrography_sources['NHDPlus HR']\n",
    "\n",
    "#\n",
    "# This demo uses a few datasets that have been clipped out of larger, national\n",
    "# datasets and are distributed with the code.  This is simply to save download\n",
    "# time for this simple problem and to lower the barrier for trying out\n",
    "# Watershed Workflow.  A more typical workflow would delete these lines (as \n",
    "# these files would not exist for other watersheds).\n",
    "#\n",
    "# The default versions of these download large raster and shapefile files that\n",
    "# are defined over a very large region (globally or the entire US).\n",
    "#\n",
    "# DELETE THIS SECTION for non-Coweeta runs\n",
    "dtb_file = os.path.join(data_dir, 'DTB', 'DTB.tif')\n",
    "geo_file = os.path.join(data_dir, 'GLHYMPS', 'GLHYMPS.shp')\n",
    "\n",
    "# GLHYMPs is a several-GB download, so we have sliced it and included the slice here\n",
    "sources['geologic structure'] = watershed_workflow.sources.ManagerGLHYMPS(geo_file)\n",
    "\n",
    "# The Pelletier DTB map is not particularly accurate at Coweeta -- the SoilGrids map seems to be better.\n",
    "# Here we will use a clipped version of that map.\n",
    "sources['depth to bedrock'] = watershed_workflow.sources.ManagerRaster(dtb_file)\n",
    "\n",
    "# END DELETE THIS SECTION\n",
    "\n",
    "# log the sources that will be used here\n",
    "watershed_workflow.sources.logSources(sources)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5457ce61",
   "metadata": {},
   "source": [
    "# Basin Geometry\n",
    "\n",
    "In this section, we choose the basin, the streams to be included in the stream-aligned mesh, and make sure that all are resolved discretely at appropriate length scales for this work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f208035c-0731-4537-ad22-a7119e1c3a77",
   "metadata": {},
   "source": [
    "## the Watershed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct and plot the WW object used for storing watersheds\n",
    "watershed = watershed_workflow.split_hucs.SplitHUCs(coweeta)\n",
    "watershed.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Training cell **\n",
    "# # methods and attributes\n",
    "# for f in dir(watershed):\n",
    "#    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362788be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Training cell **\n",
    "# watershed\n",
    "# watershed.df\n",
    "# watershed.exterior\n",
    "# watershed.linestrings\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3619f352",
   "metadata": {},
   "source": [
    "DN: not sure if it makes sense to go over handled collection since we have only a single watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a65bd",
   "metadata": {},
   "source": [
    "## the Rivers "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee38b81",
   "metadata": {},
   "source": [
    "** Training Note **\n",
    "<b> Construction </b>\n",
    "Rivers are constructed using a collection of reaches using one of the following methods:\n",
    "\n",
    "1) `geometry` looks at coincident coordinates. This is needed when working with non-NHD data.\n",
    "2) `hydroseq` is valid only for NHDPlus data. This method uses the NHDPlus VAA tables Hydrologic Sequence. \n",
    "3) `native` reads a natively dumped list of rivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5828ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download/collect the river network within that shape's bounds\n",
    "reaches = sources['hydrography'].getShapesByGeometry(watershed.exterior, crs, out_crs=crs)\n",
    "rivers = watershed_workflow.river_tree.createRivers(reaches, method='hydroseq') # other method: 'geometry'\n",
    "watershed_orig, rivers_orig = watershed, rivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Training cell **\n",
    "#reaches \n",
    "#reaches.iloc[0]\n",
    "#dict(reaches.iloc[0])\n",
    "#reaches.iloc[0].geometry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a30d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517566da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the rivers and watershed\n",
    "def plot(ws, rivs, ax=None):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "    ws.plot(color='k', marker='+', markersize=10, ax=ax)\n",
    "    for river in rivs:\n",
    "        river.plot(marker='x', markersize=10, ax=ax)\n",
    "\n",
    "plot(watershed, rivers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070d526d",
   "metadata": {},
   "source": [
    "### ----- Training Cells: River Tree Features -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e1e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Training cell **\n",
    "river = rivers[0]\n",
    "# river.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedbc66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "# A `River` is a composed of nodes. A single node in the River is also a `River` object, representing one reach and its upstream children.\n",
    "\n",
    "# Traversing tree\n",
    "# Basic tree traversal uses the `preOrder()` method, which is a depth-first traversal of the tree.\n",
    "nodes = [node for node in river.preOrder()]\n",
    "for node in nodes:\n",
    "    print(node.properties['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "river.plot(ax=ax, color='b')\n",
    "# node = nodes[3]\n",
    "# node.plot(ax=ax, color='r', linestyle='-')  ##### CHANGE TO 2, 3, .. and march up the river #####\n",
    "# ax.plot(*node.linestring.xy, color='g', linestyle='--')\n",
    "ax.set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Training cell ** N\n",
    "# Parent/Child Relationships\n",
    "node = river # nodes[3]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "node.plot(ax=ax, color='b', linestyle='-')\n",
    "node.children[0].plot(ax=ax, color='r', linestyle='-')\n",
    "node.children[1].plot(ax=ax, color='g', linestyle='-')\n",
    "# ax.plot(*node.linestring.xy, color='g', linestyle='--')\n",
    "ax.set_aspect('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3517689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can access siblings \n",
    "node = nodes[1]\n",
    "\n",
    "siblings = list(node.siblings); print(siblings)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3,3))\n",
    "node.plot(color='b', ax=ax)\n",
    "siblings[0].plot(ax=ax, color='r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9df0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can access parent node\n",
    "parent = node.parent\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3,3))\n",
    "node.plot(color='b', ax=ax)\n",
    "ax.plot(*parent.linestring.xy, color='r', linestyle='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7336074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods \n",
    "node.linestring.length\n",
    "\n",
    "rivers[0].linestring.length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b9869",
   "metadata": {},
   "source": [
    "### ------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f525e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the originals for plotting comparisons\n",
    "def createCopy(watershed, rivers):\n",
    "    \"\"\"To compare before/after, we often want to create copies.  Note in real workflows most things are done in-place without copies.\"\"\"\n",
    "    return watershed.deepcopy(), [r.deepcopy() for r in rivers]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f9cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** Training cell ** \n",
    "# refinement controls\n",
    "refine_d0 = 200\n",
    "refine_d1 = 600\n",
    "\n",
    "refine_L0 = 125\n",
    "refine_L1 = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7c249",
   "metadata": {},
   "source": [
    "Looking at the river resampling strategies in the code, here's a concise summary:\n",
    "\n",
    "The watershed workflow provides multiple strategies for resampling river networks, allowing users to control the resolution and detail of river representations based on different criteria and requirements.\n",
    "\n",
    "• **Fixed Length**: Uses a single uniform target segment length across all river reaches for consistent resolution\n",
    "\n",
    "• **Property-Based**: Reads target lengths from each reach's stored properties, enabling different resolutions for different river segments\n",
    "\n",
    "• **Function-Driven**: Employs custom functions to dynamically compute target lengths with min/max bounds, offering maximum flexibility for complex logic\n",
    "\n",
    "• **Distance-Adaptive**: Varies resolution based on proximity to reference shapes, providing finer detail near important features and coarser detail elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bca4b3-9265-41b9-bf67-44fcd247c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "watershed, rivers = createCopy(watershed_orig, rivers_orig)\n",
    "\n",
    "# simplifying -- this sets the discrete length scale of both the watershed boundary and the rivers\n",
    "watershed_workflow.simplify(watershed, rivers, refine_L0, refine_L1, refine_d0, refine_d1)\n",
    "\n",
    "# simplify may remove reaches from the rivers object\n",
    "# -- this call removes any reaches from the dataframe as well, signaling we are all done removing reaches\n",
    "#\n",
    "# ETC: NOTE -- can this be moved into the simplify call?\n",
    "for river in rivers:\n",
    "    river.resetDataFrame()\n",
    "\n",
    "# Now that the river network is set, find the watershed boundary outlets\n",
    "for river in rivers:\n",
    "    watershed_workflow.hydrography.findOutletsByCrossings(watershed, river)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97dcf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(watershed, rivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc15fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this generates a zoomable map, showing different reaches and watersheds, \n",
    "# with discrete points.  Problem areas are clickable to get IDs for manual\n",
    "# modifications.\n",
    "m = watershed.explore(marker=False)\n",
    "for river in rivers_orig:\n",
    "    m = river.explore(m=m, column=None, color='black', name=river['name']+' raw', marker=False)\n",
    "for river in rivers:\n",
    "    m = river.explore(m=m)\n",
    "    \n",
    "m = watershed_workflow.makeMap(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216a36d4-17d6-4d3d-af0b-e10beb87f7c7",
   "metadata": {},
   "source": [
    "## Mesh Geometry\n",
    "\n",
    "Now we create stream-aligned mesh conforming to the above discretization of river tree and watershed boundary. River width can be provided either using a dictionary stream-order:width, or width can be assigned as a property for each node, which can be read on the fly while assigning width of quad elements in the river mesh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine triangles if they get too acute\n",
    "min_angle = 32 # degrees\n",
    "\n",
    "# width of reach by stream order (order:width)\n",
    "widths = dict({1:8,2:12,3:16})\n",
    "\n",
    "# create the mesh\n",
    "m2, areas, dists = watershed_workflow.tessalateRiverAligned(watershed, rivers, \n",
    "                                                            river_width=widths,\n",
    "                                                            refine_min_angle=min_angle,\n",
    "                                                            refine_distance=[refine_d0, refine_A0, refine_d1, refine_A1],\n",
    "                                                            diagnostics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb82a7",
   "metadata": {},
   "source": [
    "### ----- Training Cells: Tessellation Options --------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508e0231",
   "metadata": {},
   "source": [
    "<b> Internal Boundaries and Refinement </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a99067",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_d0 = 200\n",
    "refine_d1 = 600\n",
    "\n",
    "refine_A0 = refine_L0**2 / 2\n",
    "refine_A1 = refine_L1**2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc73d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import box\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon as mplPolygon\n",
    "\n",
    "# Create a box shape using shapely's box function\n",
    "box_shape = box(1445315, -646934, 1445805, -646458)\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(2.5,2.5))\n",
    "\n",
    "# Plot the box shape using ax.add_patch\n",
    "box_patch = mplPolygon(list(box_shape.exterior.coords), closed=True, edgecolor='r', linewidth=1, alpha=0.7, facecolor='none')\n",
    "ax.add_patch(box_patch)\n",
    "\n",
    "# Plot the watershed and rivers\n",
    "watershed.plot(ax=ax, alpha=0.5, color='k', linewidth=1)\n",
    "rivers[0].plot(ax=ax, color='b', linewidth=0.5)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine triangles if they get too acute\n",
    "min_angle = 32 # degrees\n",
    "\n",
    "# width of reach by stream order (order:width)\n",
    "widths = dict({1:8,2:12,3:16})\n",
    "\n",
    "# create the mesh\n",
    "m2, areas, dists = watershed_workflow.tessalateRiverAligned(watershed, rivers, \n",
    "                                                            river_width=widths,\n",
    "                                                            refine_min_angle=min_angle,\n",
    "                                                            # refine_polygons = [[box_shape], 500],\n",
    "                                                            # internal_boundaries = [box_shape],\n",
    "                                                            refine_distance=[refine_d0, refine_A0, refine_d1, refine_A1],\n",
    "                                                            diagnostics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting surface mesh with elevations\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Plot the main mesh without a color bar\n",
    "mp = m2.plot(facecolor='darkgray', edgecolor='w', ax=ax, linewidth=0.5, colorbar=False)\n",
    "ax.set_title('Surface Mesh')\n",
    "ax.set_aspect('equal', 'datalim')\n",
    "watershed.plot(ax=ax, alpha=0.5, color='k', linewidth=1)\n",
    "\n",
    "# rivers[0].plot(ax=ax, color='red', linewidth=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72296dae",
   "metadata": {},
   "source": [
    "### ----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8e73c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get a raster for the elevation map, based on 3DEP\n",
    "dem = sources['DEM'].getDataset(watershed.exterior.buffer(100), watershed.crs)['dem']\n",
    "\n",
    "# provide surface mesh elevations\n",
    "watershed_workflow.elevate(m2, dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa06a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the DEM raster\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the DEM data\n",
    "im = dem.plot(ax=ax, cmap='terrain', add_colorbar=False)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "cbar.set_label('Elevation (m)', rotation=270, labelpad=15)\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title('Digital Elevation Model (DEM)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "\n",
    "# Set equal aspect ratio\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0756b890",
   "metadata": {},
   "source": [
    "In the pit-filling algorithm, we want to make sure that river corridor is not filled up. Hence we exclude river corridor cells from the pit-filling algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hydrologically condition the mesh, removing pits\n",
    "river_mask=np.zeros((len(m2.conn)))\n",
    "for i, elem in enumerate(m2.conn):\n",
    "    if not len(elem)==3:\n",
    "        river_mask[i]=1     \n",
    "watershed_workflow.condition.fillPitsDual(m2, is_waterbody=river_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681cc9d6",
   "metadata": {},
   "source": [
    "There are a range of options to condition river corridor mesh. We hydrologically condition the river mesh, ensuring unimpeded water flow in river corridors by globally adjusting flowlines to rectify artificial obstructions from inconsistent DEM elevations or misalignments. Please read the documentation for more information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditioning river mesh\n",
    "#\n",
    "# adding elevations to the river tree for stream bed conditioning\n",
    "watershed_workflow.condition.setProfileByDEM(rivers, dem)\n",
    "\n",
    "# conditioning the river mesh using NHD elevations\n",
    "watershed_workflow.condition.conditionRiverMesh(m2, rivers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf32e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting surface mesh with elevations\n",
    "fig, ax = plt.subplots()\n",
    "ax2 = ax.inset_axes([0.65,0.05,0.3,0.5])\n",
    "cbax = fig.add_axes([0.05,0.05,0.9,0.05])\n",
    "\n",
    "mp = m2.plot(facecolors='elevation', edgecolors=None, ax=ax, linewidth=0.5, colorbar=False)\n",
    "cbar = fig.colorbar(mp, orientation=\"horizontal\", cax=cbax)\n",
    "ax.set_title('surface mesh with elevations')\n",
    "ax.set_aspect('equal', 'datalim')\n",
    "\n",
    "mp2 = m2.plot(facecolors='elevation', edgecolors='white', ax=ax2, colorbar=False)\n",
    "ax2.set_aspect('equal', 'datalim')\n",
    "\n",
    "xlim = (1.4433e6, 1.4438e6)\n",
    "ylim = (-647000, -647500)\n",
    "\n",
    "ax2.set_xlim(xlim)\n",
    "ax2.set_ylim(ylim)\n",
    "ax2.set_xticks([])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "ax.indicate_inset_zoom(ax2, edgecolor='k')\n",
    "\n",
    "cbar.ax.set_title('elevation [m]')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136c055",
   "metadata": {},
   "source": [
    "## River/Stream-specific LabeledSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labeled sets for subcatchments and outlets\n",
    "watershed_workflow.regions.addWatershedAndOutletRegions(m2, watershed, outlet_width=250, exterior_outlet=True)\n",
    "\n",
    "# add labeled sets for river corridor cells\n",
    "watershed_workflow.regions.addRiverCorridorRegions(m2, rivers)\n",
    "\n",
    "# add labeled sets for river corridor cells by order\n",
    "watershed_workflow.regions.addStreamOrderRegions(m2, rivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92519786",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ls in m2.labeled_sets:\n",
    "    print(f'{ls.setid} : {ls.entity} : {len(ls.ent_ids)} : \"{ls.name}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e78caca",
   "metadata": {},
   "source": [
    "<b> This concludes part 1 of the workflow, in which spatial discretization of the watershed and river network is performed, and surface mesh geometry is created. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de0fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "intermediate_dir = './intermediate_files/'\n",
    "\n",
    "# Ensure the intermediate directory exists\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "# Save m2 and watershed objects using pickle\n",
    "with open(os.path.join(intermediate_dir, 'm2.pkl'), 'wb') as f:\n",
    "    pickle.dump(m2, f)\n",
    "\n",
    "with open(os.path.join(intermediate_dir, 'watershed.pkl'), 'wb') as f:\n",
    "    pickle.dump(watershed, f)\n",
    "\n",
    "# Concatenate river dataframes and save as parquet\n",
    "river_df = pd.concat([river.to_dataframe() for river in rivers])\n",
    "river_df.to_parquet(os.path.join(intermediate_dir, 'rivers.parquet'))\n",
    "\n",
    "print(\"Intermediate files saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cac0f95-826e-413b-bec8-dd046df21ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python3 (watershed_workflow)",
   "language": "python",
   "name": "watershed_workflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 685.28134,
   "end_time": "2022-03-07T16:00:23.770205",
   "environment_variables": {},
   "exception": true,
   "input_path": "full_workflow_master.ipynb",
   "output_path": "full_workflow_EastTaylor.ipynb",
   "parameters": {
    "hucs": "[14020001,]",
    "name": "EastTaylor",
    "prune_by_area_fraction": 0.005
   },
   "start_time": "2022-03-07T15:48:58.488865",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
